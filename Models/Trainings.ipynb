{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ePpzqho-Nr",
        "outputId": "fe5cc05d-0356-4bed-ec8d-4dbf1063ad5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - loss: 0.2975 - mae: 0.4134 - val_loss: 0.1881 - val_mae: 0.3259 - learning_rate: 3.0000e-04\n",
            "Epoch 2/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2017 - mae: 0.3408 - val_loss: 0.1585 - val_mae: 0.2987 - learning_rate: 3.0000e-04\n",
            "Epoch 3/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1754 - mae: 0.3185 - val_loss: 0.1351 - val_mae: 0.2743 - learning_rate: 3.0000e-04\n",
            "Epoch 4/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1618 - mae: 0.3055 - val_loss: 0.1206 - val_mae: 0.2614 - learning_rate: 3.0000e-04\n",
            "Epoch 5/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1461 - mae: 0.2905 - val_loss: 0.1130 - val_mae: 0.2510 - learning_rate: 3.0000e-04\n",
            "Epoch 6/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1359 - mae: 0.2800 - val_loss: 0.1045 - val_mae: 0.2420 - learning_rate: 3.0000e-04\n",
            "Epoch 7/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1300 - mae: 0.2737 - val_loss: 0.0992 - val_mae: 0.2356 - learning_rate: 3.0000e-04\n",
            "Epoch 8/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1205 - mae: 0.2624 - val_loss: 0.0921 - val_mae: 0.2267 - learning_rate: 3.0000e-04\n",
            "Epoch 9/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1154 - mae: 0.2569 - val_loss: 0.0949 - val_mae: 0.2295 - learning_rate: 3.0000e-04\n",
            "Epoch 10/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1126 - mae: 0.2537 - val_loss: 0.0854 - val_mae: 0.2182 - learning_rate: 3.0000e-04\n",
            "Epoch 11/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1075 - mae: 0.2481 - val_loss: 0.0801 - val_mae: 0.2103 - learning_rate: 3.0000e-04\n",
            "Epoch 12/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1019 - mae: 0.2421 - val_loss: 0.0780 - val_mae: 0.2079 - learning_rate: 3.0000e-04\n",
            "Epoch 13/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1000 - mae: 0.2390 - val_loss: 0.0759 - val_mae: 0.2046 - learning_rate: 3.0000e-04\n",
            "Epoch 14/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0984 - mae: 0.2362 - val_loss: 0.0715 - val_mae: 0.1978 - learning_rate: 3.0000e-04\n",
            "Epoch 15/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0968 - mae: 0.2339 - val_loss: 0.0714 - val_mae: 0.1974 - learning_rate: 3.0000e-04\n",
            "Epoch 16/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0912 - mae: 0.2275 - val_loss: 0.0697 - val_mae: 0.1949 - learning_rate: 3.0000e-04\n",
            "Epoch 17/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0893 - mae: 0.2251 - val_loss: 0.0665 - val_mae: 0.1915 - learning_rate: 3.0000e-04\n",
            "Epoch 18/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0880 - mae: 0.2230 - val_loss: 0.0643 - val_mae: 0.1867 - learning_rate: 3.0000e-04\n",
            "Epoch 19/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0860 - mae: 0.2210 - val_loss: 0.0630 - val_mae: 0.1842 - learning_rate: 3.0000e-04\n",
            "Epoch 20/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0841 - mae: 0.2176 - val_loss: 0.0612 - val_mae: 0.1826 - learning_rate: 3.0000e-04\n",
            "Epoch 21/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0825 - mae: 0.2161 - val_loss: 0.0599 - val_mae: 0.1800 - learning_rate: 3.0000e-04\n",
            "Epoch 22/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0808 - mae: 0.2131 - val_loss: 0.0586 - val_mae: 0.1774 - learning_rate: 3.0000e-04\n",
            "Epoch 23/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0788 - mae: 0.2106 - val_loss: 0.0573 - val_mae: 0.1754 - learning_rate: 3.0000e-04\n",
            "Epoch 24/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0777 - mae: 0.2090 - val_loss: 0.0573 - val_mae: 0.1749 - learning_rate: 3.0000e-04\n",
            "Epoch 25/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0742 - mae: 0.2046 - val_loss: 0.0552 - val_mae: 0.1727 - learning_rate: 3.0000e-04\n",
            "Epoch 26/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0758 - mae: 0.2059 - val_loss: 0.0542 - val_mae: 0.1702 - learning_rate: 3.0000e-04\n",
            "Epoch 27/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0735 - mae: 0.2033 - val_loss: 0.0534 - val_mae: 0.1692 - learning_rate: 3.0000e-04\n",
            "Epoch 28/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0726 - mae: 0.2019 - val_loss: 0.0535 - val_mae: 0.1701 - learning_rate: 3.0000e-04\n",
            "Epoch 29/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0696 - mae: 0.1975 - val_loss: 0.0514 - val_mae: 0.1648 - learning_rate: 3.0000e-04\n",
            "Epoch 30/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0698 - mae: 0.1984 - val_loss: 0.0517 - val_mae: 0.1661 - learning_rate: 3.0000e-04\n",
            "Epoch 31/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0676 - mae: 0.1951 - val_loss: 0.0499 - val_mae: 0.1635 - learning_rate: 3.0000e-04\n",
            "Epoch 32/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0685 - mae: 0.1955 - val_loss: 0.0494 - val_mae: 0.1631 - learning_rate: 3.0000e-04\n",
            "Epoch 33/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0654 - mae: 0.1912 - val_loss: 0.0492 - val_mae: 0.1618 - learning_rate: 3.0000e-04\n",
            "Epoch 34/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0654 - mae: 0.1915 - val_loss: 0.0480 - val_mae: 0.1596 - learning_rate: 3.0000e-04\n",
            "Epoch 35/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0659 - mae: 0.1915 - val_loss: 0.0454 - val_mae: 0.1550 - learning_rate: 3.0000e-04\n",
            "Epoch 36/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0650 - mae: 0.1905 - val_loss: 0.0456 - val_mae: 0.1552 - learning_rate: 3.0000e-04\n",
            "Epoch 37/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0641 - mae: 0.1893 - val_loss: 0.0447 - val_mae: 0.1539 - learning_rate: 3.0000e-04\n",
            "Epoch 38/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0626 - mae: 0.1873 - val_loss: 0.0435 - val_mae: 0.1510 - learning_rate: 3.0000e-04\n",
            "Epoch 39/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0628 - mae: 0.1869 - val_loss: 0.0466 - val_mae: 0.1587 - learning_rate: 3.0000e-04\n",
            "Epoch 40/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0634 - mae: 0.1878 - val_loss: 0.0442 - val_mae: 0.1528 - learning_rate: 3.0000e-04\n",
            "Epoch 41/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0616 - mae: 0.1851 - val_loss: 0.0439 - val_mae: 0.1517 - learning_rate: 3.0000e-04\n",
            "Epoch 42/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0617 - mae: 0.1848 - val_loss: 0.0432 - val_mae: 0.1516 - learning_rate: 3.0000e-04\n",
            "Epoch 43/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0607 - mae: 0.1836 - val_loss: 0.0421 - val_mae: 0.1488 - learning_rate: 3.0000e-04\n",
            "Epoch 44/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0601 - mae: 0.1830 - val_loss: 0.0420 - val_mae: 0.1495 - learning_rate: 3.0000e-04\n",
            "Epoch 45/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0591 - mae: 0.1808 - val_loss: 0.0411 - val_mae: 0.1467 - learning_rate: 3.0000e-04\n",
            "Epoch 46/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0588 - mae: 0.1807 - val_loss: 0.0424 - val_mae: 0.1498 - learning_rate: 3.0000e-04\n",
            "Epoch 47/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0586 - mae: 0.1808 - val_loss: 0.0426 - val_mae: 0.1479 - learning_rate: 3.0000e-04\n",
            "Epoch 48/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0581 - mae: 0.1795 - val_loss: 0.0399 - val_mae: 0.1447 - learning_rate: 3.0000e-04\n",
            "Epoch 49/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0573 - mae: 0.1782 - val_loss: 0.0405 - val_mae: 0.1460 - learning_rate: 3.0000e-04\n",
            "Epoch 50/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0557 - mae: 0.1754 - val_loss: 0.0383 - val_mae: 0.1415 - learning_rate: 3.0000e-04\n",
            "Epoch 51/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0558 - mae: 0.1756 - val_loss: 0.0393 - val_mae: 0.1439 - learning_rate: 3.0000e-04\n",
            "Epoch 52/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0559 - mae: 0.1759 - val_loss: 0.0389 - val_mae: 0.1425 - learning_rate: 3.0000e-04\n",
            "Epoch 53/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0555 - mae: 0.1751 - val_loss: 0.0384 - val_mae: 0.1408 - learning_rate: 3.0000e-04\n",
            "Epoch 54/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0552 - mae: 0.1743 - val_loss: 0.0374 - val_mae: 0.1400 - learning_rate: 3.0000e-04\n",
            "Epoch 55/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0553 - mae: 0.1747 - val_loss: 0.0361 - val_mae: 0.1365 - learning_rate: 3.0000e-04\n",
            "Epoch 56/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0536 - mae: 0.1721 - val_loss: 0.0380 - val_mae: 0.1402 - learning_rate: 3.0000e-04\n",
            "Epoch 57/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0539 - mae: 0.1727 - val_loss: 0.0377 - val_mae: 0.1408 - learning_rate: 3.0000e-04\n",
            "Epoch 58/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0527 - mae: 0.1710 - val_loss: 0.0371 - val_mae: 0.1385 - learning_rate: 3.0000e-04\n",
            "Epoch 59/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0544 - mae: 0.1727 - val_loss: 0.0362 - val_mae: 0.1369 - learning_rate: 3.0000e-04\n",
            "Epoch 60/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0523 - mae: 0.1704 - val_loss: 0.0359 - val_mae: 0.1360 - learning_rate: 3.0000e-04\n",
            "Epoch 61/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0510 - mae: 0.1678 - val_loss: 0.0354 - val_mae: 0.1347 - learning_rate: 3.0000e-04\n",
            "Epoch 62/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0525 - mae: 0.1700 - val_loss: 0.0362 - val_mae: 0.1373 - learning_rate: 3.0000e-04\n",
            "Epoch 63/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0512 - mae: 0.1680 - val_loss: 0.0345 - val_mae: 0.1330 - learning_rate: 3.0000e-04\n",
            "Epoch 64/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0515 - mae: 0.1680 - val_loss: 0.0351 - val_mae: 0.1334 - learning_rate: 3.0000e-04\n",
            "Epoch 65/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0511 - mae: 0.1673 - val_loss: 0.0346 - val_mae: 0.1330 - learning_rate: 3.0000e-04\n",
            "Epoch 66/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0504 - mae: 0.1665 - val_loss: 0.0357 - val_mae: 0.1349 - learning_rate: 3.0000e-04\n",
            "Epoch 67/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0496 - mae: 0.1655 - val_loss: 0.0348 - val_mae: 0.1340 - learning_rate: 3.0000e-04\n",
            "Epoch 68/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - mae: 0.1647 - val_loss: 0.0342 - val_mae: 0.1329 - learning_rate: 3.0000e-04\n",
            "Epoch 69/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0494 - mae: 0.1647 - val_loss: 0.0343 - val_mae: 0.1323 - learning_rate: 3.0000e-04\n",
            "Epoch 70/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0492 - mae: 0.1646 - val_loss: 0.0348 - val_mae: 0.1336 - learning_rate: 3.0000e-04\n",
            "Epoch 71/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0502 - mae: 0.1659 - val_loss: 0.0336 - val_mae: 0.1315 - learning_rate: 3.0000e-04\n",
            "Epoch 72/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - mae: 0.1648 - val_loss: 0.0332 - val_mae: 0.1303 - learning_rate: 3.0000e-04\n",
            "Epoch 73/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0484 - mae: 0.1635 - val_loss: 0.0332 - val_mae: 0.1307 - learning_rate: 3.0000e-04\n",
            "Epoch 74/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - mae: 0.1634 - val_loss: 0.0349 - val_mae: 0.1329 - learning_rate: 3.0000e-04\n",
            "Epoch 75/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0493 - mae: 0.1646 - val_loss: 0.0325 - val_mae: 0.1292 - learning_rate: 3.0000e-04\n",
            "Epoch 76/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0479 - mae: 0.1624 - val_loss: 0.0323 - val_mae: 0.1297 - learning_rate: 3.0000e-04\n",
            "Epoch 77/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0485 - mae: 0.1632 - val_loss: 0.0324 - val_mae: 0.1281 - learning_rate: 3.0000e-04\n",
            "Epoch 78/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0477 - mae: 0.1614 - val_loss: 0.0318 - val_mae: 0.1270 - learning_rate: 3.0000e-04\n",
            "Epoch 79/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0473 - mae: 0.1610 - val_loss: 0.0316 - val_mae: 0.1273 - learning_rate: 3.0000e-04\n",
            "Epoch 80/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0475 - mae: 0.1613 - val_loss: 0.0320 - val_mae: 0.1275 - learning_rate: 3.0000e-04\n",
            "Epoch 81/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0475 - mae: 0.1609 - val_loss: 0.0313 - val_mae: 0.1266 - learning_rate: 3.0000e-04\n",
            "Epoch 82/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0463 - mae: 0.1591 - val_loss: 0.0331 - val_mae: 0.1303 - learning_rate: 3.0000e-04\n",
            "Epoch 83/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0475 - mae: 0.1607 - val_loss: 0.0310 - val_mae: 0.1262 - learning_rate: 3.0000e-04\n",
            "Epoch 84/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0462 - mae: 0.1593 - val_loss: 0.0312 - val_mae: 0.1256 - learning_rate: 3.0000e-04\n",
            "Epoch 85/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0463 - mae: 0.1593 - val_loss: 0.0310 - val_mae: 0.1247 - learning_rate: 3.0000e-04\n",
            "Epoch 86/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0473 - mae: 0.1598 - val_loss: 0.0305 - val_mae: 0.1238 - learning_rate: 3.0000e-04\n",
            "Epoch 87/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0457 - mae: 0.1576 - val_loss: 0.0321 - val_mae: 0.1277 - learning_rate: 3.0000e-04\n",
            "Epoch 88/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0459 - mae: 0.1582 - val_loss: 0.0306 - val_mae: 0.1247 - learning_rate: 3.0000e-04\n",
            "Epoch 89/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0464 - mae: 0.1590 - val_loss: 0.0316 - val_mae: 0.1266 - learning_rate: 3.0000e-04\n",
            "Epoch 90/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0459 - mae: 0.1578 - val_loss: 0.0314 - val_mae: 0.1261 - learning_rate: 3.0000e-04\n",
            "Epoch 91/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0453 - mae: 0.1578 - val_loss: 0.0302 - val_mae: 0.1239 - learning_rate: 3.0000e-04\n",
            "Epoch 92/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0452 - mae: 0.1565 - val_loss: 0.0308 - val_mae: 0.1248 - learning_rate: 3.0000e-04\n",
            "Epoch 93/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0440 - mae: 0.1554 - val_loss: 0.0293 - val_mae: 0.1222 - learning_rate: 3.0000e-04\n",
            "Epoch 94/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0453 - mae: 0.1569 - val_loss: 0.0301 - val_mae: 0.1228 - learning_rate: 3.0000e-04\n",
            "Epoch 95/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0447 - mae: 0.1554 - val_loss: 0.0301 - val_mae: 0.1236 - learning_rate: 3.0000e-04\n",
            "Epoch 96/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0449 - mae: 0.1563 - val_loss: 0.0295 - val_mae: 0.1221 - learning_rate: 3.0000e-04\n",
            "Epoch 97/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0456 - mae: 0.1573 - val_loss: 0.0302 - val_mae: 0.1231 - learning_rate: 3.0000e-04\n",
            "Epoch 98/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0442 - mae: 0.1550 - val_loss: 0.0312 - val_mae: 0.1257 - learning_rate: 3.0000e-04\n",
            "Epoch 99/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.1555 - val_loss: 0.0298 - val_mae: 0.1223 - learning_rate: 3.0000e-04\n",
            "Epoch 100/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0431 - mae: 0.1538 - val_loss: 0.0291 - val_mae: 0.1203 - learning_rate: 3.0000e-04\n",
            "Epoch 101/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0435 - mae: 0.1537 - val_loss: 0.0296 - val_mae: 0.1218 - learning_rate: 3.0000e-04\n",
            "Epoch 102/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0442 - mae: 0.1547 - val_loss: 0.0299 - val_mae: 0.1224 - learning_rate: 3.0000e-04\n",
            "Epoch 103/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0433 - mae: 0.1529 - val_loss: 0.0307 - val_mae: 0.1232 - learning_rate: 3.0000e-04\n",
            "Epoch 104/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0443 - mae: 0.1547 - val_loss: 0.0293 - val_mae: 0.1218 - learning_rate: 3.0000e-04\n",
            "Epoch 105/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0432 - mae: 0.1528 - val_loss: 0.0291 - val_mae: 0.1204 - learning_rate: 3.0000e-04\n",
            "Epoch 106/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0431 - mae: 0.1529 - val_loss: 0.0299 - val_mae: 0.1225 - learning_rate: 3.0000e-04\n",
            "Epoch 107/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0434 - mae: 0.1528 - val_loss: 0.0286 - val_mae: 0.1200 - learning_rate: 3.0000e-04\n",
            "Epoch 108/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0428 - mae: 0.1523 - val_loss: 0.0294 - val_mae: 0.1212 - learning_rate: 3.0000e-04\n",
            "Epoch 109/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0426 - mae: 0.1522 - val_loss: 0.0288 - val_mae: 0.1196 - learning_rate: 3.0000e-04\n",
            "Epoch 110/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0420 - mae: 0.1507 - val_loss: 0.0283 - val_mae: 0.1184 - learning_rate: 3.0000e-04\n",
            "Epoch 111/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0430 - mae: 0.1520 - val_loss: 0.0279 - val_mae: 0.1179 - learning_rate: 3.0000e-04\n",
            "Epoch 112/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0422 - mae: 0.1509 - val_loss: 0.0281 - val_mae: 0.1185 - learning_rate: 3.0000e-04\n",
            "Epoch 113/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0421 - mae: 0.1502 - val_loss: 0.0282 - val_mae: 0.1186 - learning_rate: 3.0000e-04\n",
            "Epoch 114/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0423 - mae: 0.1505 - val_loss: 0.0287 - val_mae: 0.1198 - learning_rate: 3.0000e-04\n",
            "Epoch 115/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0415 - mae: 0.1498 - val_loss: 0.0295 - val_mae: 0.1200 - learning_rate: 3.0000e-04\n",
            "Epoch 116/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0414 - mae: 0.1497 - val_loss: 0.0274 - val_mae: 0.1160 - learning_rate: 3.0000e-04\n",
            "Epoch 117/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0416 - mae: 0.1501 - val_loss: 0.0285 - val_mae: 0.1194 - learning_rate: 3.0000e-04\n",
            "Epoch 118/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0419 - mae: 0.1506 - val_loss: 0.0289 - val_mae: 0.1203 - learning_rate: 3.0000e-04\n",
            "Epoch 119/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0412 - mae: 0.1494 - val_loss: 0.0275 - val_mae: 0.1163 - learning_rate: 3.0000e-04\n",
            "Epoch 120/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0410 - mae: 0.1488 - val_loss: 0.0285 - val_mae: 0.1179 - learning_rate: 3.0000e-04\n",
            "Epoch 121/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0417 - mae: 0.1499 - val_loss: 0.0277 - val_mae: 0.1178 - learning_rate: 3.0000e-04\n",
            "Epoch 122/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0405 - mae: 0.1478 - val_loss: 0.0272 - val_mae: 0.1167 - learning_rate: 3.0000e-04\n",
            "Epoch 123/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0410 - mae: 0.1487 - val_loss: 0.0274 - val_mae: 0.1161 - learning_rate: 3.0000e-04\n",
            "Epoch 124/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0406 - mae: 0.1486 - val_loss: 0.0268 - val_mae: 0.1151 - learning_rate: 3.0000e-04\n",
            "Epoch 125/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0410 - mae: 0.1486 - val_loss: 0.0284 - val_mae: 0.1187 - learning_rate: 3.0000e-04\n",
            "Epoch 126/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0405 - mae: 0.1474 - val_loss: 0.0281 - val_mae: 0.1181 - learning_rate: 3.0000e-04\n",
            "Epoch 127/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0406 - mae: 0.1477 - val_loss: 0.0271 - val_mae: 0.1157 - learning_rate: 3.0000e-04\n",
            "Epoch 128/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0405 - mae: 0.1477 - val_loss: 0.0278 - val_mae: 0.1173 - learning_rate: 3.0000e-04\n",
            "Epoch 129/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0409 - mae: 0.1479 - val_loss: 0.0276 - val_mae: 0.1164 - learning_rate: 3.0000e-04\n",
            "Epoch 130/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0406 - mae: 0.1474 - val_loss: 0.0275 - val_mae: 0.1158 - learning_rate: 3.0000e-04\n",
            "Epoch 131/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0402 - mae: 0.1468 - val_loss: 0.0270 - val_mae: 0.1148 - learning_rate: 3.0000e-04\n",
            "Epoch 132/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0405 - mae: 0.1475 - val_loss: 0.0264 - val_mae: 0.1135 - learning_rate: 3.0000e-04\n",
            "Epoch 133/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0400 - mae: 0.1467 - val_loss: 0.0278 - val_mae: 0.1164 - learning_rate: 3.0000e-04\n",
            "Epoch 134/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0403 - mae: 0.1472 - val_loss: 0.0264 - val_mae: 0.1139 - learning_rate: 3.0000e-04\n",
            "Epoch 135/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0393 - mae: 0.1451 - val_loss: 0.0267 - val_mae: 0.1142 - learning_rate: 3.0000e-04\n",
            "Epoch 136/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0397 - mae: 0.1460 - val_loss: 0.0270 - val_mae: 0.1151 - learning_rate: 3.0000e-04\n",
            "Epoch 137/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0397 - mae: 0.1459 - val_loss: 0.0262 - val_mae: 0.1131 - learning_rate: 3.0000e-04\n",
            "Epoch 138/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0392 - mae: 0.1451 - val_loss: 0.0268 - val_mae: 0.1144 - learning_rate: 3.0000e-04\n",
            "Epoch 139/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0392 - mae: 0.1451 - val_loss: 0.0271 - val_mae: 0.1153 - learning_rate: 3.0000e-04\n",
            "Epoch 140/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0399 - mae: 0.1460 - val_loss: 0.0264 - val_mae: 0.1132 - learning_rate: 3.0000e-04\n",
            "Epoch 141/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0387 - mae: 0.1440 - val_loss: 0.0271 - val_mae: 0.1151 - learning_rate: 3.0000e-04\n",
            "Epoch 142/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0402 - mae: 0.1464 - val_loss: 0.0269 - val_mae: 0.1149 - learning_rate: 3.0000e-04\n",
            "Epoch 143/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0393 - mae: 0.1445 - val_loss: 0.0270 - val_mae: 0.1143 - learning_rate: 3.0000e-04\n",
            "Epoch 144/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0393 - mae: 0.1449 - val_loss: 0.0271 - val_mae: 0.1147 - learning_rate: 3.0000e-04\n",
            "Epoch 145/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0390 - mae: 0.1444 - val_loss: 0.0266 - val_mae: 0.1133 - learning_rate: 3.0000e-04\n",
            "Epoch 146/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0368 - mae: 0.1402 - val_loss: 0.0243 - val_mae: 0.1079 - learning_rate: 1.5000e-04\n",
            "Epoch 147/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0358 - mae: 0.1379 - val_loss: 0.0242 - val_mae: 0.1073 - learning_rate: 1.5000e-04\n",
            "Epoch 148/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0358 - mae: 0.1382 - val_loss: 0.0239 - val_mae: 0.1069 - learning_rate: 1.5000e-04\n",
            "Epoch 149/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0360 - mae: 0.1382 - val_loss: 0.0237 - val_mae: 0.1062 - learning_rate: 1.5000e-04\n",
            "Epoch 150/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0351 - mae: 0.1368 - val_loss: 0.0239 - val_mae: 0.1066 - learning_rate: 1.5000e-04\n",
            "Epoch 151/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0362 - mae: 0.1386 - val_loss: 0.0234 - val_mae: 0.1052 - learning_rate: 1.5000e-04\n",
            "Epoch 152/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0349 - mae: 0.1366 - val_loss: 0.0240 - val_mae: 0.1069 - learning_rate: 1.5000e-04\n",
            "Epoch 153/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0347 - mae: 0.1360 - val_loss: 0.0234 - val_mae: 0.1053 - learning_rate: 1.5000e-04\n",
            "Epoch 154/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0352 - mae: 0.1371 - val_loss: 0.0238 - val_mae: 0.1067 - learning_rate: 1.5000e-04\n",
            "Epoch 155/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0352 - mae: 0.1369 - val_loss: 0.0233 - val_mae: 0.1054 - learning_rate: 1.5000e-04\n",
            "Epoch 156/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0347 - mae: 0.1355 - val_loss: 0.0237 - val_mae: 0.1056 - learning_rate: 1.5000e-04\n",
            "Epoch 157/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0354 - mae: 0.1369 - val_loss: 0.0231 - val_mae: 0.1046 - learning_rate: 1.5000e-04\n",
            "Epoch 158/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0348 - mae: 0.1357 - val_loss: 0.0234 - val_mae: 0.1048 - learning_rate: 1.5000e-04\n",
            "Epoch 159/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0354 - mae: 0.1367 - val_loss: 0.0240 - val_mae: 0.1069 - learning_rate: 1.5000e-04\n",
            "Epoch 160/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0345 - mae: 0.1354 - val_loss: 0.0235 - val_mae: 0.1053 - learning_rate: 1.5000e-04\n",
            "Epoch 161/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0349 - mae: 0.1371 - val_loss: 0.0236 - val_mae: 0.1057 - learning_rate: 1.5000e-04\n",
            "Epoch 162/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0350 - mae: 0.1365 - val_loss: 0.0238 - val_mae: 0.1060 - learning_rate: 1.5000e-04\n",
            "Epoch 163/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0345 - mae: 0.1361 - val_loss: 0.0233 - val_mae: 0.1052 - learning_rate: 1.5000e-04\n",
            "Epoch 164/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0346 - mae: 0.1362 - val_loss: 0.0231 - val_mae: 0.1045 - learning_rate: 1.5000e-04\n",
            "Epoch 165/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0338 - mae: 0.1347 - val_loss: 0.0230 - val_mae: 0.1043 - learning_rate: 1.5000e-04\n",
            "Epoch 166/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0342 - mae: 0.1351 - val_loss: 0.0239 - val_mae: 0.1065 - learning_rate: 1.5000e-04\n",
            "Epoch 167/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0343 - mae: 0.1350 - val_loss: 0.0235 - val_mae: 0.1052 - learning_rate: 1.5000e-04\n",
            "Epoch 168/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0349 - mae: 0.1364 - val_loss: 0.0235 - val_mae: 0.1050 - learning_rate: 1.5000e-04\n",
            "Epoch 169/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0340 - mae: 0.1344 - val_loss: 0.0231 - val_mae: 0.1045 - learning_rate: 1.5000e-04\n",
            "Epoch 170/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0344 - mae: 0.1347 - val_loss: 0.0232 - val_mae: 0.1047 - learning_rate: 1.5000e-04\n",
            "Epoch 171/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0337 - mae: 0.1340 - val_loss: 0.0231 - val_mae: 0.1043 - learning_rate: 1.5000e-04\n",
            "Epoch 172/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0335 - mae: 0.1338 - val_loss: 0.0236 - val_mae: 0.1057 - learning_rate: 1.5000e-04\n",
            "Epoch 173/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0341 - mae: 0.1343 - val_loss: 0.0233 - val_mae: 0.1053 - learning_rate: 1.5000e-04\n",
            "Epoch 174/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0329 - mae: 0.1325 - val_loss: 0.0225 - val_mae: 0.1024 - learning_rate: 7.5000e-05\n",
            "Epoch 175/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0325 - mae: 0.1314 - val_loss: 0.0222 - val_mae: 0.1019 - learning_rate: 7.5000e-05\n",
            "Epoch 176/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0327 - mae: 0.1322 - val_loss: 0.0219 - val_mae: 0.1009 - learning_rate: 7.5000e-05\n",
            "Epoch 177/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0327 - mae: 0.1319 - val_loss: 0.0224 - val_mae: 0.1020 - learning_rate: 7.5000e-05\n",
            "Epoch 178/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0321 - mae: 0.1301 - val_loss: 0.0223 - val_mae: 0.1015 - learning_rate: 7.5000e-05\n",
            "Epoch 179/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0316 - mae: 0.1294 - val_loss: 0.0219 - val_mae: 0.1011 - learning_rate: 7.5000e-05\n",
            "Epoch 180/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0320 - mae: 0.1303 - val_loss: 0.0220 - val_mae: 0.1015 - learning_rate: 7.5000e-05\n",
            "Epoch 181/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0325 - mae: 0.1317 - val_loss: 0.0218 - val_mae: 0.1003 - learning_rate: 7.5000e-05\n",
            "Epoch 182/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0331 - mae: 0.1323 - val_loss: 0.0219 - val_mae: 0.1014 - learning_rate: 7.5000e-05\n",
            "Epoch 183/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0323 - mae: 0.1310 - val_loss: 0.0219 - val_mae: 0.1014 - learning_rate: 7.5000e-05\n",
            "Epoch 184/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0329 - mae: 0.1326 - val_loss: 0.0213 - val_mae: 0.0994 - learning_rate: 7.5000e-05\n",
            "Epoch 185/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0327 - mae: 0.1315 - val_loss: 0.0222 - val_mae: 0.1017 - learning_rate: 7.5000e-05\n",
            "Epoch 186/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0327 - mae: 0.1310 - val_loss: 0.0218 - val_mae: 0.1007 - learning_rate: 7.5000e-05\n",
            "Epoch 187/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0330 - mae: 0.1319 - val_loss: 0.0215 - val_mae: 0.0999 - learning_rate: 7.5000e-05\n",
            "Epoch 188/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0312 - mae: 0.1289 - val_loss: 0.0217 - val_mae: 0.1010 - learning_rate: 7.5000e-05\n",
            "Epoch 189/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0322 - mae: 0.1312 - val_loss: 0.0214 - val_mae: 0.1003 - learning_rate: 7.5000e-05\n",
            "Epoch 190/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0321 - mae: 0.1301 - val_loss: 0.0214 - val_mae: 0.0999 - learning_rate: 7.5000e-05\n",
            "Epoch 191/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0320 - mae: 0.1309 - val_loss: 0.0214 - val_mae: 0.0998 - learning_rate: 7.5000e-05\n",
            "Epoch 192/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0317 - mae: 0.1301 - val_loss: 0.0215 - val_mae: 0.1002 - learning_rate: 7.5000e-05\n",
            "Epoch 193/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0319 - mae: 0.1301 - val_loss: 0.0214 - val_mae: 0.1000 - learning_rate: 3.7500e-05\n",
            "Epoch 194/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0317 - mae: 0.1297 - val_loss: 0.0211 - val_mae: 0.0992 - learning_rate: 3.7500e-05\n",
            "Epoch 195/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0307 - mae: 0.1278 - val_loss: 0.0209 - val_mae: 0.0987 - learning_rate: 3.7500e-05\n",
            "Epoch 196/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0313 - mae: 0.1289 - val_loss: 0.0210 - val_mae: 0.0987 - learning_rate: 3.7500e-05\n",
            "Epoch 197/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0318 - mae: 0.1297 - val_loss: 0.0207 - val_mae: 0.0979 - learning_rate: 3.7500e-05\n",
            "Epoch 198/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0314 - mae: 0.1293 - val_loss: 0.0210 - val_mae: 0.0986 - learning_rate: 3.7500e-05\n",
            "Epoch 199/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0313 - mae: 0.1288 - val_loss: 0.0209 - val_mae: 0.0985 - learning_rate: 3.7500e-05\n",
            "Epoch 200/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0307 - mae: 0.1272 - val_loss: 0.0208 - val_mae: 0.0981 - learning_rate: 3.7500e-05\n",
            "Epoch 201/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0312 - mae: 0.1292 - val_loss: 0.0209 - val_mae: 0.0986 - learning_rate: 3.7500e-05\n",
            "Epoch 202/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0310 - mae: 0.1279 - val_loss: 0.0210 - val_mae: 0.0990 - learning_rate: 3.7500e-05\n",
            "Epoch 203/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0311 - mae: 0.1284 - val_loss: 0.0210 - val_mae: 0.0988 - learning_rate: 3.7500e-05\n",
            "Epoch 204/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0306 - mae: 0.1279 - val_loss: 0.0210 - val_mae: 0.0985 - learning_rate: 3.7500e-05\n",
            "Epoch 205/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0313 - mae: 0.1284 - val_loss: 0.0208 - val_mae: 0.0979 - learning_rate: 3.7500e-05\n",
            "Epoch 206/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0316 - mae: 0.1294 - val_loss: 0.0208 - val_mae: 0.0981 - learning_rate: 1.8750e-05\n",
            "Epoch 207/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1270 - val_loss: 0.0207 - val_mae: 0.0978 - learning_rate: 1.8750e-05\n",
            "Epoch 208/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0306 - mae: 0.1276 - val_loss: 0.0208 - val_mae: 0.0983 - learning_rate: 1.8750e-05\n",
            "Epoch 209/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0305 - mae: 0.1270 - val_loss: 0.0208 - val_mae: 0.0978 - learning_rate: 1.8750e-05\n",
            "Epoch 210/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1278 - val_loss: 0.0208 - val_mae: 0.0982 - learning_rate: 1.8750e-05\n",
            "Epoch 211/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0313 - mae: 0.1290 - val_loss: 0.0207 - val_mae: 0.0974 - learning_rate: 1.8750e-05\n",
            "Epoch 212/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1266 - val_loss: 0.0206 - val_mae: 0.0975 - learning_rate: 1.8750e-05\n",
            "Epoch 213/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1280 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 1.8750e-05\n",
            "Epoch 214/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0307 - mae: 0.1275 - val_loss: 0.0206 - val_mae: 0.0978 - learning_rate: 1.8750e-05\n",
            "Epoch 215/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0308 - mae: 0.1274 - val_loss: 0.0208 - val_mae: 0.0981 - learning_rate: 1.8750e-05\n",
            "Epoch 216/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1279 - val_loss: 0.0208 - val_mae: 0.0981 - learning_rate: 1.8750e-05\n",
            "Epoch 217/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0310 - mae: 0.1279 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 1.8750e-05\n",
            "Epoch 218/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0305 - mae: 0.1276 - val_loss: 0.0205 - val_mae: 0.0973 - learning_rate: 1.8750e-05\n",
            "Epoch 219/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0303 - mae: 0.1268 - val_loss: 0.0205 - val_mae: 0.0973 - learning_rate: 1.8750e-05\n",
            "Epoch 220/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0309 - mae: 0.1277 - val_loss: 0.0205 - val_mae: 0.0971 - learning_rate: 1.8750e-05\n",
            "Epoch 221/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0311 - mae: 0.1284 - val_loss: 0.0207 - val_mae: 0.0975 - learning_rate: 1.8750e-05\n",
            "Epoch 222/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0305 - mae: 0.1270 - val_loss: 0.0207 - val_mae: 0.0978 - learning_rate: 9.3750e-06\n",
            "Epoch 223/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1280 - val_loss: 0.0208 - val_mae: 0.0979 - learning_rate: 9.3750e-06\n",
            "Epoch 224/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0311 - mae: 0.1282 - val_loss: 0.0206 - val_mae: 0.0976 - learning_rate: 9.3750e-06\n",
            "Epoch 225/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0312 - mae: 0.1285 - val_loss: 0.0206 - val_mae: 0.0973 - learning_rate: 9.3750e-06\n",
            "Epoch 226/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0306 - mae: 0.1271 - val_loss: 0.0205 - val_mae: 0.0974 - learning_rate: 9.3750e-06\n",
            "Epoch 227/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1276 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 9.3750e-06\n",
            "Epoch 228/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0309 - mae: 0.1276 - val_loss: 0.0206 - val_mae: 0.0974 - learning_rate: 9.3750e-06\n",
            "Epoch 229/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1267 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 9.3750e-06\n",
            "Epoch 230/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1264 - val_loss: 0.0206 - val_mae: 0.0973 - learning_rate: 4.6875e-06\n",
            "Epoch 231/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1263 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 4.6875e-06\n",
            "Epoch 232/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0303 - mae: 0.1268 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 4.6875e-06\n",
            "Epoch 233/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1279 - val_loss: 0.0203 - val_mae: 0.0968 - learning_rate: 4.6875e-06\n",
            "Epoch 234/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0306 - mae: 0.1279 - val_loss: 0.0203 - val_mae: 0.0966 - learning_rate: 4.6875e-06\n",
            "Epoch 235/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1268 - val_loss: 0.0204 - val_mae: 0.0967 - learning_rate: 4.6875e-06\n",
            "Epoch 236/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0303 - mae: 0.1267 - val_loss: 0.0204 - val_mae: 0.0967 - learning_rate: 4.6875e-06\n",
            "Epoch 237/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0298 - mae: 0.1256 - val_loss: 0.0204 - val_mae: 0.0971 - learning_rate: 4.6875e-06\n",
            "Epoch 238/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0305 - mae: 0.1269 - val_loss: 0.0205 - val_mae: 0.0973 - learning_rate: 4.6875e-06\n",
            "Epoch 239/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0309 - mae: 0.1277 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 4.6875e-06\n",
            "Epoch 240/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1268 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 2.3438e-06\n",
            "Epoch 241/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0298 - mae: 0.1262 - val_loss: 0.0203 - val_mae: 0.0968 - learning_rate: 2.3438e-06\n",
            "Epoch 242/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0306 - mae: 0.1273 - val_loss: 0.0205 - val_mae: 0.0971 - learning_rate: 2.3438e-06\n",
            "Epoch 243/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1271 - val_loss: 0.0204 - val_mae: 0.0968 - learning_rate: 2.3438e-06\n",
            "Epoch 244/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1267 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 2.3438e-06\n",
            "Epoch 245/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0306 - mae: 0.1273 - val_loss: 0.0204 - val_mae: 0.0968 - learning_rate: 2.3438e-06\n",
            "Epoch 246/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0304 - mae: 0.1267 - val_loss: 0.0203 - val_mae: 0.0968 - learning_rate: 2.3438e-06\n",
            "Epoch 247/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0298 - mae: 0.1259 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 2.3438e-06\n",
            "Epoch 248/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0307 - mae: 0.1273 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 1.1719e-06\n",
            "Epoch 249/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0294 - mae: 0.1255 - val_loss: 0.0205 - val_mae: 0.0975 - learning_rate: 1.1719e-06\n",
            "Epoch 250/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0312 - mae: 0.1283 - val_loss: 0.0203 - val_mae: 0.0968 - learning_rate: 1.1719e-06\n",
            "Epoch 251/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0302 - mae: 0.1265 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 1.1719e-06\n",
            "Epoch 252/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0300 - mae: 0.1263 - val_loss: 0.0203 - val_mae: 0.0966 - learning_rate: 1.1719e-06\n",
            "Epoch 253/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0305 - mae: 0.1274 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 1.1719e-06\n",
            "Epoch 254/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0300 - mae: 0.1260 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 1.1719e-06\n",
            "Epoch 255/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0301 - mae: 0.1265 - val_loss: 0.0205 - val_mae: 0.0975 - learning_rate: 1.1719e-06\n",
            "Epoch 256/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0304 - mae: 0.1269 - val_loss: 0.0204 - val_mae: 0.0969 - learning_rate: 1.1719e-06\n",
            "Epoch 257/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0305 - mae: 0.1272 - val_loss: 0.0203 - val_mae: 0.0965 - learning_rate: 1.1719e-06\n",
            "Epoch 258/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1271 - val_loss: 0.0205 - val_mae: 0.0971 - learning_rate: 1.1719e-06\n",
            "Epoch 259/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0300 - mae: 0.1267 - val_loss: 0.0204 - val_mae: 0.0972 - learning_rate: 1.1719e-06\n",
            "Epoch 260/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1268 - val_loss: 0.0205 - val_mae: 0.0973 - learning_rate: 1.1719e-06\n",
            "Epoch 261/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1268 - val_loss: 0.0203 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
            "Epoch 262/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0302 - mae: 0.1271 - val_loss: 0.0204 - val_mae: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 263/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1273 - val_loss: 0.0205 - val_mae: 0.0974 - learning_rate: 1.0000e-06\n",
            "Epoch 264/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1264 - val_loss: 0.0203 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
            "Epoch 265/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0306 - mae: 0.1272 - val_loss: 0.0203 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
            "Epoch 266/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1268 - val_loss: 0.0205 - val_mae: 0.0973 - learning_rate: 1.0000e-06\n",
            "Epoch 267/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1269 - val_loss: 0.0203 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
            "Epoch 268/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1268 - val_loss: 0.0205 - val_mae: 0.0973 - learning_rate: 1.0000e-06\n",
            "Epoch 269/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0305 - mae: 0.1274 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 1.0000e-06\n",
            "Epoch 270/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1259 - val_loss: 0.0204 - val_mae: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 271/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1274 - val_loss: 0.0207 - val_mae: 0.0979 - learning_rate: 1.0000e-06\n",
            "Epoch 272/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1266 - val_loss: 0.0204 - val_mae: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 273/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0300 - mae: 0.1262 - val_loss: 0.0203 - val_mae: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 274/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1266 - val_loss: 0.0205 - val_mae: 0.0973 - learning_rate: 1.0000e-06\n",
            "Epoch 275/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0300 - mae: 0.1262 - val_loss: 0.0203 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
            "Epoch 276/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0308 - mae: 0.1275 - val_loss: 0.0204 - val_mae: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 277/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1270 - val_loss: 0.0202 - val_mae: 0.0962 - learning_rate: 1.0000e-06\n",
            "Epoch 278/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0299 - mae: 0.1261 - val_loss: 0.0203 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
            "Epoch 279/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1266 - val_loss: 0.0206 - val_mae: 0.0974 - learning_rate: 1.0000e-06\n",
            "Epoch 280/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1265 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 1.0000e-06\n",
            "Epoch 281/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1265 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 1.0000e-06\n",
            "Epoch 282/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0300 - mae: 0.1263 - val_loss: 0.0204 - val_mae: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 283/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0309 - mae: 0.1275 - val_loss: 0.0207 - val_mae: 0.0979 - learning_rate: 1.0000e-06\n",
            "Epoch 284/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1268 - val_loss: 0.0203 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
            "Epoch 285/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1268 - val_loss: 0.0204 - val_mae: 0.0971 - learning_rate: 1.0000e-06\n",
            "Epoch 286/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0306 - mae: 0.1270 - val_loss: 0.0205 - val_mae: 0.0974 - learning_rate: 1.0000e-06\n",
            "Epoch 287/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1266 - val_loss: 0.0205 - val_mae: 0.0971 - learning_rate: 1.0000e-06\n",
            "Epoch 288/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1271 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 1.0000e-06\n",
            "Epoch 289/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1268 - val_loss: 0.0203 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
            "Epoch 290/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1264 - val_loss: 0.0204 - val_mae: 0.0970 - learning_rate: 1.0000e-06\n",
            "Epoch 291/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0301 - mae: 0.1259 - val_loss: 0.0204 - val_mae: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 292/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0303 - mae: 0.1271 - val_loss: 0.0204 - val_mae: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 293/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0299 - mae: 0.1260 - val_loss: 0.0204 - val_mae: 0.0971 - learning_rate: 1.0000e-06\n",
            "Epoch 294/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0302 - mae: 0.1265 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 1.0000e-06\n",
            "Epoch 295/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0307 - mae: 0.1276 - val_loss: 0.0203 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
            "Epoch 296/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0304 - mae: 0.1270 - val_loss: 0.0205 - val_mae: 0.0972 - learning_rate: 1.0000e-06\n",
            "Epoch 297/300\n",
            "\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0306 - mae: 0.1273 - val_loss: 0.0203 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Cargar dataset\n",
        "# ============================================\n",
        "df = pd.read_csv(\"/Datasets/solar_merged_clean.csv\")\n",
        "\n",
        "# ============================================\n",
        "# Crear transformaciones geograficas\n",
        "# ============================================\n",
        "\n",
        "# convertir grados a radianes\n",
        "df[\"lat_rad\"] = np.radians(df[\"lat\"])\n",
        "df[\"lon_rad\"] = np.radians(df[\"lon\"])\n",
        "\n",
        "df[\"sin_lat\"] = np.sin(df[\"lat_rad\"])\n",
        "df[\"cos_lat\"] = np.cos(df[\"lat_rad\"])\n",
        "df[\"sin_lon\"] = np.sin(df[\"lon_rad\"])\n",
        "df[\"cos_lon\"] = np.cos(df[\"lon_rad\"])\n",
        "\n",
        "# ============================================\n",
        "# Crear transformaciones temporales\n",
        "# ============================================\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "\n",
        "df[\"dayofyear\"] = df[\"date\"].dt.dayofyear\n",
        "df[\"dayofyear_norm\"] = df[\"dayofyear\"] / 365.0\n",
        "\n",
        "df[\"sin_doy\"] = np.sin(2 * np.pi * df[\"dayofyear\"] / 365.0)\n",
        "df[\"cos_doy\"] = np.cos(2 * np.pi * df[\"dayofyear\"] / 365.0)\n",
        "\n",
        "# ============================================\n",
        "# Variables derivadas\n",
        "# ============================================\n",
        "\n",
        "# lag 1 del target\n",
        "df[\"surface_net_solar_radiation_sum_lag1\"] = (\n",
        "    df[\"surface_net_solar_radiation_sum\"].shift(1)\n",
        ")\n",
        "\n",
        "# indice combinando temp y humedad\n",
        "df[\"temp_humidity_index\"] = (\n",
        "    df[\"temperature_2m_C\"] * (df[\"relative_humidity\"] / 100.0)\n",
        ")\n",
        "\n",
        "# razon entre nubes y presion\n",
        "df[\"cloud_pressure_ratio\"] = (\n",
        "    df[\"Cloud_Cover_Mean_24h\"] / df[\"surface_pressure\"]\n",
        ")\n",
        "\n",
        "# eliminar filas con NaN creados por el lag\n",
        "df = df.dropna()\n",
        "\n",
        "# ============================================\n",
        "# Seleccionar features finales\n",
        "# ============================================\n",
        "feature_cols = [\n",
        "    \"Cloud_Cover_Mean_24h\",\n",
        "    \"relative_humidity\",\n",
        "    \"temperature_2m_C\",\n",
        "    \"total_precipitation_sum\",\n",
        "    \"surface_pressure\",\n",
        "    \"elevation\",\n",
        "    \"sin_lat\",\n",
        "    \"cos_lat\",\n",
        "    \"sin_lon\",\n",
        "    \"cos_lon\",\n",
        "\n",
        "    \"sin_doy\",\n",
        "    \"cos_doy\",\n",
        "    \"dayofyear_norm\",\n",
        "\n",
        "    \"surface_net_solar_radiation_sum_lag1\",\n",
        "    \"temp_humidity_index\",\n",
        "    \"cloud_pressure_ratio\"\n",
        "]\n",
        "\n",
        "target_col = \"surface_net_solar_radiation_sum\"\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[target_col].values\n",
        "\n",
        "# ============================================\n",
        "# Split train / val / test\n",
        "# ============================================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# Escalar\n",
        "# ============================================\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ============================================\n",
        "# Normalizar target\n",
        "# ============================================\n",
        "y_mean = y_train.mean()\n",
        "y_std = y_train.std()\n",
        "\n",
        "y_train_n = (y_train - y_mean) / y_std\n",
        "y_val_n = (y_val - y_mean) / y_std\n",
        "y_test_n = (y_test - y_mean) / y_std\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Build model\n",
        "# ================================\n",
        "def build_model(input_dim):\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # capa inicial\n",
        "    model.add(layers.Dense(\n",
        "        128,\n",
        "        kernel_regularizer=regularizers.l2(1e-6),\n",
        "        input_shape=(input_dim,)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation(\"gelu\"))\n",
        "\n",
        "    # bloque 1\n",
        "    model.add(layers.Dense(\n",
        "        256,\n",
        "        kernel_regularizer=regularizers.l2(1e-6)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation(\"gelu\"))\n",
        "    model.add(layers.Dropout(0.1))\n",
        "\n",
        "    # bloque 2 tipo residual\n",
        "    model.add(layers.Dense(\n",
        "        256,\n",
        "        kernel_regularizer=regularizers.l2(1e-6)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation(\"gelu\"))\n",
        "\n",
        "    model.add(layers.Dense(\n",
        "        128,\n",
        "        kernel_regularizer=regularizers.l2(1e-6)\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation(\"gelu\"))\n",
        "    model.add(layers.Dropout(0.05))\n",
        "\n",
        "    # bloque final\n",
        "    model.add(layers.Dense(64, activation=\"gelu\"))\n",
        "    model.add(layers.Dense(32, activation=\"gelu\"))\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
        "        loss=\"mse\",\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "model = build_model(X_train_scaled.shape[1])\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=20, restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",\n",
        "        factor=0.5,\n",
        "        patience=8,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Train model\n",
        "# ================================\n",
        "history = model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train_n,\n",
        "    validation_data=(X_val_scaled, y_val_n),\n",
        "    epochs=300,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1QFJNdQqRFO",
        "outputId": "7e804bb1-21da-423a-8507-5846e59b6aa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['solar_scaler.pkl']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save(\"solar_model.keras\")\n",
        "import joblib\n",
        "joblib.dump(scaler, \"solar_scaler.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "_2gA-UQMC_L-",
        "outputId": "5689714f-6868-42cb-e2e7-976cb1ac2c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "===== METRICAS REALES DEL MODELO =====\n",
            "MAE real : 334064.76246466924\n",
            "RMSE real: 479379.41741120286\n",
            "R2 real  : 0.980617448630675\n",
            "MAPE (%) : 32.41255809904362\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================\n",
        "# Predicciones normalizadas\n",
        "# ============================================\n",
        "y_pred_n = model.predict(X_test_scaled)\n",
        "\n",
        "# ============================================\n",
        "# Des-normalizar (volver a la escala real)\n",
        "# ============================================\n",
        "y_pred = y_pred_n * y_std + y_mean\n",
        "y_real = y_test_n * 0  # evitar errores si no existe\n",
        "y_real = y_test  # los valores reales ya estan sin normalizar\n",
        "\n",
        "# ============================================\n",
        "# Metricas reales\n",
        "# ============================================\n",
        "mae_real = mean_absolute_error(y_real, y_pred)\n",
        "rmse_real = np.sqrt(mean_squared_error(y_real, y_pred))\n",
        "r2_real = r2_score(y_real, y_pred)\n",
        "mape_real = np.mean(np.abs((y_real - y_pred) / (y_real + 1e-6))) * 100\n",
        "\n",
        "print(\"===== METRICAS REALES DEL MODELO =====\")\n",
        "print(\"MAE real :\", mae_real)\n",
        "print(\"RMSE real:\", rmse_real)\n",
        "print(\"R2 real  :\", r2_real)\n",
        "print(\"MAPE (%) :\", mape_real)\n",
        "\n",
        "# ============================================\n",
        "# Grafico: Real vs Pred\n",
        "# ============================================\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_real, y_pred, s=4)\n",
        "min_val = min(y_real.min(), y_pred.min())\n",
        "max_val = max(y_real.max(), y_pred.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], \"--\")\n",
        "plt.xlabel(\"Real\")\n",
        "plt.ylabel(\"Prediccion\")\n",
        "plt.title(\"Real vs Prediccion\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# Distribucion del error\n",
        "# ============================================\n",
        "error = y_real - y_pred\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.hist(error, bins=50)\n",
        "plt.title(\"Distribucion del Error\")\n",
        "plt.xlabel(\"Error (real - pred)\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
